{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "generator"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(os.walk(\"../data/interim/generated\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('../data/interim/generated', [], ['data_{51}_to_{100}.json', 'data_{101}_to_{150}.json', 'data_{1}_to_{50}.json'])\n"
     ]
    }
   ],
   "source": [
    "for i in os.walk(\"../data/interim/generated\"):\n",
    "    print(i,end=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "001_to_50.json\n",
      "051_t0_100.json\n",
      "101_to_150.json\n"
     ]
    }
   ],
   "source": [
    "for i in sorted(os.listdir(\"/home/jeet/Documents/Jeet/FT_LLM/data/interim/generated\")):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "posix.ScandirIterator"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(os.scandir(\"/home/jeet/Documents/Jeet/FT_LLM/data/interim/generated\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<DirEntry '101_to_150.json'>\n",
      "<DirEntry '051_t0_100.json'>\n",
      "<DirEntry '001_to_50.json'>\n"
     ]
    }
   ],
   "source": [
    "for i in os.scandir(\"../data/interim/generated\"):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101_to_150.json\n",
      "051_t0_100.json\n",
      "001_to_50.json\n"
     ]
    }
   ],
   "source": [
    "with os.scandir(\"../data/interim/generated\") as entries:\n",
    "    for entry in entries:\n",
    "        print(entry.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_{101}_to_{150}.json\n",
      "data_{1}_to_{50}.json\n",
      "data_{51}_to_{100}.json\n"
     ]
    }
   ],
   "source": [
    "path = \"\"\"../data/interim/generated/\"\"\"\n",
    "for i in sorted(os.listdir(path)):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['data_{101}_to_{150}.json', 'data_{1}_to_{50}.json', 'data_{51}_to_{100}.json']\n"
     ]
    }
   ],
   "source": [
    "items = os.listdir(path)\n",
    "soreted_items = sorted(items)\n",
    "print(soreted_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "j\n",
      "jd\n",
      "shlf\n",
      "kdhfsd\n"
     ]
    }
   ],
   "source": [
    "mylist = [\"j\",\"shlf\",\"kdhfsd\",\"jd\"]\n",
    "\n",
    "for i in sorted(mylist,key=len):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Person:\n",
    "    def __init__(self) -> None:\n",
    "        pass\n",
    "\n",
    "    def walk(self):\n",
    "        print(\"person is walking\")\n",
    "    def talk(self):\n",
    "        print(\"person is talking\")\n",
    "\n",
    "\n",
    "class Jeet(Person):\n",
    "    def __init__(self) -> None:\n",
    "        # super().__init__()?\n",
    "        pass\n",
    "\n",
    "    def walk(self):\n",
    "        print(\"jeet is walking\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "person is walking\n"
     ]
    }
   ],
   "source": [
    "p = Person()\n",
    "p.walk()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "person is talking\n"
     ]
    }
   ],
   "source": [
    "j = Jeet()\n",
    "j.talk()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/pythia-70m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [2577, 1416, 310, 5139, 292], 'attention_mask': [1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "text = \"my name is jeet\"\n",
    "decoded_text = tokenizer(text)\n",
    "print(decoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['myname', 'is', 'jeet', 90, 345, 34]\n"
     ]
    }
   ],
   "source": [
    "thisList = [\"myname\",\"is\",\"jeet\"]\n",
    "thatlist = [90,345,34]\n",
    "\n",
    "final_list = thisList.extend(thatlist)\n",
    "print(final_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "def extend_list(l1,l2):\n",
    "    return l1.extend(l2)\n",
    "\n",
    "final_list = extend_list(thisList,thatlist)\n",
    "print(final_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/pythia-70m\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"../dataset.jsonl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "instruction_dataset_df = pandas.read_json(filename,lines=True)\n",
    "examples = instruction_dataset_df.to_dict() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"converted.txt\",\"w\") as file:\n",
    "    file.write(str(examples['input'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Are you ready to build your dream home in one of Melbourne's most sought-after communities? This EAST facing 448 square meter vacant land at 15 Caspian Street, Bonnie Brook, within the prestigious Woodlea Estate, presents an incredible opportunity for savvy buyers looking for the perfect canvas to design and build their ideal home.\n",
      "\n",
      "Key Features:\n",
      "\n",
      "Generous Land Size: With a spacious 448 square meters to work with, there's ample room to create a stunning residence that suits your lifestyle and preferences.\n",
      "\n",
      "Woodlea Estate: Woodlea is renowned for its community-oriented living, offering a vibrant and family-friendly atmosphere. You'll have access to parks, playgrounds, schools, shopping, and more, making it an ideal place to call home.\n",
      "Woodlea has been carefully designed to connect residents with everything they need. Every home is within 200m of a park, with every park and neighbourhood able to be accessed via various walkable routes. With work and education facilities on the doorstep and easy access to bus and train routes, opportunities abound to shop, work and study close to home.\n",
      "\n",
      "The ultimate vision for Woodlea is a community teeming with an enviable array of modern sporting and recreation facilities, an asset for both the local community and the surrounding district. Open active space abounds, with native woodlands and wetlands, parklands, adventure playgrounds and pocket parks all situated close to home.\n",
      "\n",
      "From easy access to sporting facilities, quality schools, shops and convenient transport to the inclusion of unique landscaping packages, Woodlea has been designed for a modern, fulfilling lifestyle.\n",
      "\n",
      "Location: Located in Bonnie Brook, you'll enjoy the serenity of a suburban environment while being within easy reach of Tarneit, Melbourne CBD, and major transportation routes.\n",
      "\n",
      "Endless Possibilities: This land is a blank canvas, allowing you to create a home that perfectly suits your needs. Whether it's a modern family haven, a stylish townhouse, or an investment property, the possibilities are endless.*\n",
      "\n",
      "Don't Miss Out!\n",
      "\n",
      "Opportunities like this don't come around often. Whether you're a first-time buyer, a savvy investor, or someone looking for a fresh start in a vibrant community,\n",
      "        15 CASPIAN STREET, BONNIE BROOK is the blank canvas you've been waiting for.\n",
      "\n",
      "Contact Nikhil Jude Dsouza on 0421 037 906 today to discuss the endless possibilities that await you on this remarkable piece of land. Don't let this opportunity slip through your fingers-seize it today and build your future in Fraser Rise!\n",
      "\n",
      "DISCLAIMER: All stated dimensions are approximate only. Particulars given are for general information only and do not constitute any representation on the part of the vendor or agent.\n",
      "['Built in Robes', 'Outdoor Entertainment']\n"
     ]
    }
   ],
   "source": [
    "print(examples['input'][0])\n",
    "print(examples['output'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = examples['input'][0] + str(examples['output'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Are you ready to build your dream home in one of Melbourne's most sought-after communities? This EAST facing 448 square meter vacant land at 15 Caspian Street, Bonnie Brook, within the prestigious Woodlea Estate, presents an incredible opportunity for savvy buyers looking for the perfect canvas to design and build their ideal home.\\n\\nKey Features:\\n\\nGenerous Land Size: With a spacious 448 square meters to work with, there's ample room to create a stunning residence that suits your lifestyle and preferences.\\n\\nWoodlea Estate: Woodlea is renowned for its community-oriented living, offering a vibrant and family-friendly atmosphere. You'll have access to parks, playgrounds, schools, shopping, and more, making it an ideal place to call home.\\nWoodlea has been carefully designed to connect residents with everything they need. Every home is within 200m of a park, with every park and neighbourhood able to be accessed via various walkable routes. With work and education facilities on the doorstep and easy access to bus and train routes, opportunities abound to shop, work and study close to home.\\n\\nThe ultimate vision for Woodlea is a community teeming with an enviable array of modern sporting and recreation facilities, an asset for both the local community and the surrounding district. Open active space abounds, with native woodlands and wetlands, parklands, adventure playgrounds and pocket parks all situated close to home.\\n\\nFrom easy access to sporting facilities, quality schools, shops and convenient transport to the inclusion of unique landscaping packages, Woodlea has been designed for a modern, fulfilling lifestyle.\\n\\nLocation: Located in Bonnie Brook, you'll enjoy the serenity of a suburban environment while being within easy reach of Tarneit, Melbourne CBD, and major transportation routes.\\n\\nEndless Possibilities: This land is a blank canvas, allowing you to create a home that perfectly suits your needs. Whether it's a modern family haven, a stylish townhouse, or an investment property, the possibilities are endless.*\\n\\nDon't Miss Out!\\n\\nOpportunities like this don't come around often. Whether you're a first-time buyer, a savvy investor, or someone looking for a fresh start in a vibrant community,\\n        15 CASPIAN STREET, BONNIE BROOK is the blank canvas you've been waiting for.\\n\\nContact Nikhil Jude Dsouza on 0421 037 906 today to discuss the endless possibilities that await you on this remarkable piece of land. Don't let this opportunity slip through your fingers-seize it today and build your future in Fraser Rise!\\n\\nDISCLAIMER: All stated dimensions are approximate only. Particulars given are for general information only and do not constitute any representation on the part of the vendor or agent.['Built in Robes', 'Outdoor Entertainment']\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150\n"
     ]
    }
   ],
   "source": [
    "print(len(examples[\"input\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_datapoint(finetuning_dataset_dict):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/pythia-70m\")\n",
    "    description = finetuning_dataset_dict[\"input\"][0]\n",
    "    output = finetuning_dataset_dict[\"output\"][0]\n",
    "    text = description + str(output)\n",
    "\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenized_inputs = tokenizer(\n",
    "        text = text,\n",
    "        return_tensors = \"np\",\n",
    "        padding = True\n",
    "    )\n",
    "\n",
    "    max_length = min(tokenized_inputs[\"input_ids\"].shape[1], 2048)\n",
    "\n",
    "    tokenized_inputs = tokenizer(\n",
    "        text = text,\n",
    "        return_tensors = \"np\",\n",
    "        truncation = True,\n",
    "        max_length = max_length\n",
    "    )\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/150 [00:00<?, ? examples/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Map:   1%|          | 1/150 [00:00<00:44,  3.36 examples/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Map:   1%|▏         | 2/150 [00:00<00:43,  3.41 examples/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Map:   2%|▏         | 3/150 [00:00<00:43,  3.40 examples/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Map:   3%|▎         | 4/150 [00:01<00:43,  3.38 examples/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Map:   3%|▎         | 5/150 [00:01<00:42,  3.41 examples/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Map:   4%|▍         | 6/150 [00:01<00:42,  3.36 examples/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Map:   5%|▍         | 7/150 [00:02<00:41,  3.41 examples/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Map:   5%|▌         | 8/150 [00:02<00:41,  3.46 examples/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Map:   6%|▌         | 9/150 [00:02<00:40,  3.48 examples/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Map:   7%|▋         | 10/150 [00:02<00:40,  3.50 examples/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Map:   7%|▋         | 11/150 [00:03<00:40,  3.47 examples/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Map:   8%|▊         | 12/150 [00:03<00:39,  3.49 examples/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Map:   9%|▊         | 13/150 [00:03<00:39,  3.46 examples/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Map:   9%|▉         | 14/150 [00:04<00:39,  3.49 examples/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Map:  10%|█         | 15/150 [00:04<00:38,  3.49 examples/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Map:  11%|█         | 16/150 [00:04<00:38,  3.47 examples/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Map:  11%|█▏        | 17/150 [00:04<00:38,  3.46 examples/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Map:  12%|█▏        | 18/150 [00:05<00:38,  3.46 examples/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Map:  13%|█▎        | 19/150 [00:05<00:39,  3.32 examples/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Map:  13%|█▎        | 20/150 [00:05<00:38,  3.36 examples/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Map:  14%|█▍        | 21/150 [00:06<00:38,  3.38 examples/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Map:  15%|█▍        | 22/150 [00:06<00:37,  3.40 examples/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Map:  15%|█▌        | 23/150 [00:06<00:36,  3.45 examples/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Map:  16%|█▌        | 24/150 [00:06<00:36,  3.47 examples/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Map:  17%|█▋        | 25/150 [00:07<00:36,  3.45 examples/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Map:  17%|█▋        | 26/150 [00:07<00:36,  3.40 examples/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Map:  18%|█▊        | 27/150 [00:07<00:35,  3.43 examples/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Map:  19%|█▊        | 28/150 [00:08<00:35,  3.46 examples/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Map:  19%|█▉        | 29/150 [00:08<00:34,  3.46 examples/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Map:  20%|██        | 30/150 [00:08<00:34,  3.46 examples/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Map:  21%|██        | 31/150 [00:09<00:35,  3.33 examples/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Map:  21%|██▏       | 32/150 [00:09<00:34,  3.38 examples/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Map:  22%|██▏       | 33/150 [00:09<00:34,  3.42 examples/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Map:  23%|██▎       | 34/150 [00:09<00:33,  3.41 examples/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Map:  23%|██▎       | 35/150 [00:10<00:33,  3.45 examples/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Map:  24%|██▍       | 36/150 [00:10<00:33,  3.42 examples/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Map:  25%|██▍       | 37/150 [00:10<00:32,  3.45 examples/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Map:  25%|██▌       | 38/150 [00:11<00:32,  3.49 examples/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Map:  26%|██▌       | 39/150 [00:11<00:31,  3.51 examples/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Map:  27%|██▋       | 40/150 [00:11<00:31,  3.48 examples/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Map:  27%|██▋       | 41/150 [00:11<00:31,  3.50 examples/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Map:  28%|██▊       | 42/150 [00:12<00:30,  3.52 examples/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Map:  29%|██▊       | 43/150 [00:12<00:30,  3.49 examples/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Map:  29%|██▉       | 44/150 [00:12<00:30,  3.49 examples/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Map:  30%|███       | 45/150 [00:13<00:30,  3.48 examples/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Map:  31%|███       | 46/150 [00:13<00:29,  3.48 examples/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Map:  31%|███▏      | 47/150 [00:13<00:29,  3.49 examples/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Map:  32%|███▏      | 48/150 [00:13<00:29,  3.49 examples/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Map:  33%|███▎      | 49/150 [00:14<00:28,  3.49 examples/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Map:  33%|███▎      | 50/150 [00:14<00:28,  3.48 examples/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Map:  34%|███▍      | 51/150 [00:14<00:28,  3.51 examples/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Map:  35%|███▍      | 52/150 [00:15<00:27,  3.53 examples/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Map:  35%|███▌      | 53/150 [00:15<00:27,  3.53 examples/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Map:  36%|███▌      | 54/150 [00:15<00:27,  3.55 examples/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Map:  37%|███▋      | 55/150 [00:15<00:26,  3.53 examples/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Map:  37%|███▋      | 56/150 [00:16<00:26,  3.55 examples/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Map:  38%|███▊      | 57/150 [00:16<00:26,  3.55 examples/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Map:  39%|███▊      | 58/150 [00:16<00:26,  3.51 examples/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Map:  39%|███▉      | 59/150 [00:17<00:26,  3.49 examples/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Map:  40%|████      | 60/150 [00:17<00:25,  3.51 examples/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Map:  41%|████      | 61/150 [00:17<00:25,  3.49 examples/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Map:  41%|████▏     | 62/150 [00:17<00:25,  3.50 examples/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Map:  42%|████▏     | 63/150 [00:18<00:24,  3.52 examples/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Map:  43%|████▎     | 64/150 [00:18<00:24,  3.52 examples/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Map:  43%|████▎     | 65/150 [00:18<00:24,  3.50 examples/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Map:  44%|████▍     | 66/150 [00:19<00:23,  3.51 examples/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Map:  45%|████▍     | 67/150 [00:19<00:25,  3.20 examples/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Map:  45%|████▌     | 68/150 [00:19<00:24,  3.30 examples/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Map:  46%|████▌     | 69/150 [00:19<00:24,  3.37 examples/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Map:  47%|████▋     | 70/150 [00:20<00:23,  3.42 examples/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Map:  47%|████▋     | 71/150 [00:20<00:23,  3.43 examples/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Map:  48%|████▊     | 72/150 [00:20<00:22,  3.43 examples/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Map:  49%|████▊     | 73/150 [00:21<00:22,  3.41 examples/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Map:  49%|████▉     | 74/150 [00:21<00:22,  3.45 examples/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Map:  50%|█████     | 75/150 [00:21<00:21,  3.46 examples/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Map:  51%|█████     | 76/150 [00:21<00:21,  3.46 examples/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Map:  51%|█████▏    | 77/150 [00:22<00:20,  3.48 examples/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Map:  52%|█████▏    | 78/150 [00:22<00:21,  3.42 examples/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Map:  53%|█████▎    | 79/150 [00:22<00:20,  3.43 examples/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Map:  53%|█████▎    | 80/150 [00:23<00:20,  3.40 examples/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Map:  54%|█████▍    | 81/150 [00:23<00:20,  3.43 examples/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Map:  55%|█████▍    | 82/150 [00:23<00:19,  3.45 examples/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Map:  55%|█████▌    | 83/150 [00:24<00:19,  3.47 examples/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Map:  56%|█████▌    | 84/150 [00:24<00:19,  3.46 examples/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Map:  57%|█████▋    | 85/150 [00:24<00:18,  3.46 examples/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Map:  57%|█████▋    | 86/150 [00:24<00:18,  3.46 examples/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Map:  58%|█████▊    | 87/150 [00:25<00:18,  3.49 examples/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Map:  59%|█████▊    | 88/150 [00:25<00:17,  3.50 examples/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Map:  59%|█████▉    | 89/150 [00:25<00:17,  3.51 examples/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Map:  60%|██████    | 90/150 [00:26<00:17,  3.50 examples/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Map:  61%|██████    | 91/150 [00:26<00:16,  3.50 examples/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Map:  61%|██████▏   | 92/150 [00:26<00:16,  3.52 examples/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Map:  62%|██████▏   | 93/150 [00:26<00:16,  3.53 examples/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Map:  63%|██████▎   | 94/150 [00:27<00:15,  3.51 examples/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Map:  63%|██████▎   | 95/150 [00:27<00:15,  3.50 examples/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Map:  64%|██████▍   | 96/150 [00:27<00:15,  3.51 examples/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Map:  65%|██████▍   | 97/150 [00:28<00:15,  3.49 examples/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Map:  65%|██████▌   | 98/150 [00:28<00:14,  3.49 examples/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Map:  66%|██████▌   | 99/150 [00:28<00:14,  3.49 examples/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Map:  67%|██████▋   | 100/150 [00:28<00:14,  3.48 examples/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Map:  67%|██████▋   | 101/150 [00:29<00:14,  3.47 examples/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Map:  68%|██████▊   | 102/150 [00:29<00:13,  3.47 examples/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Map:  69%|██████▊   | 103/150 [00:29<00:13,  3.47 examples/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Map:  69%|██████▉   | 104/150 [00:30<00:13,  3.45 examples/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Map:  70%|███████   | 105/150 [00:30<00:12,  3.47 examples/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Map:  71%|███████   | 106/150 [00:30<00:12,  3.48 examples/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Map:  71%|███████▏  | 107/150 [00:30<00:12,  3.51 examples/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Map:  72%|███████▏  | 108/150 [00:31<00:11,  3.50 examples/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Map:  73%|███████▎  | 109/150 [00:31<00:11,  3.48 examples/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Map:  73%|███████▎  | 110/150 [00:31<00:11,  3.44 examples/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Map:  74%|███████▍  | 111/150 [00:32<00:11,  3.46 examples/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Map:  75%|███████▍  | 112/150 [00:32<00:12,  3.00 examples/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Map:  75%|███████▌  | 113/150 [00:32<00:11,  3.09 examples/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Map:  76%|███████▌  | 114/150 [00:33<00:11,  3.18 examples/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Map:  77%|███████▋  | 115/150 [00:33<00:10,  3.27 examples/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Map:  77%|███████▋  | 116/150 [00:33<00:10,  3.35 examples/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Map:  78%|███████▊  | 117/150 [00:33<00:09,  3.41 examples/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Map:  79%|███████▊  | 118/150 [00:34<00:09,  3.42 examples/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Map:  79%|███████▉  | 119/150 [00:34<00:08,  3.45 examples/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Map:  80%|████████  | 120/150 [00:34<00:08,  3.45 examples/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Map:  81%|████████  | 121/150 [00:35<00:08,  3.45 examples/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Map:  81%|████████▏ | 122/150 [00:35<00:08,  3.47 examples/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Map:  82%|████████▏ | 123/150 [00:35<00:07,  3.49 examples/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Map:  83%|████████▎ | 124/150 [00:35<00:07,  3.48 examples/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Map:  83%|████████▎ | 125/150 [00:36<00:07,  3.49 examples/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Map:  84%|████████▍ | 126/150 [00:36<00:06,  3.51 examples/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Map:  85%|████████▍ | 127/150 [00:36<00:06,  3.50 examples/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Map:  85%|████████▌ | 128/150 [00:37<00:06,  3.17 examples/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Map:  86%|████████▌ | 129/150 [00:37<00:06,  3.25 examples/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Map:  87%|████████▋ | 130/150 [00:37<00:06,  3.28 examples/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Map:  87%|████████▋ | 131/150 [00:38<00:05,  3.36 examples/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Map:  88%|████████▊ | 132/150 [00:38<00:05,  3.21 examples/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Map:  89%|████████▊ | 133/150 [00:38<00:05,  3.29 examples/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Map:  89%|████████▉ | 134/150 [00:38<00:04,  3.37 examples/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Map:  90%|█████████ | 135/150 [00:39<00:04,  3.42 examples/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Map:  91%|█████████ | 136/150 [00:39<00:04,  3.44 examples/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Map:  91%|█████████▏| 137/150 [00:39<00:03,  3.46 examples/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Map:  92%|█████████▏| 138/150 [00:40<00:03,  3.48 examples/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Map:  93%|█████████▎| 139/150 [00:40<00:03,  3.34 examples/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Map:  93%|█████████▎| 140/150 [00:40<00:02,  3.39 examples/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Map:  94%|█████████▍| 141/150 [00:41<00:02,  3.43 examples/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Map:  95%|█████████▍| 142/150 [00:41<00:02,  3.43 examples/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Map:  95%|█████████▌| 143/150 [00:41<00:02,  3.45 examples/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Map:  96%|█████████▌| 144/150 [00:41<00:01,  3.40 examples/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Map:  97%|█████████▋| 145/150 [00:42<00:01,  3.43 examples/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Map:  97%|█████████▋| 146/150 [00:42<00:01,  3.43 examples/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Map:  98%|█████████▊| 147/150 [00:42<00:00,  3.40 examples/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Map:  99%|█████████▊| 148/150 [00:43<00:00,  3.25 examples/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Map:  99%|█████████▉| 149/150 [00:43<00:00,  3.32 examples/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Map: 100%|██████████| 150/150 [00:43<00:00,  3.43 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "path = \"../dataset.jsonl\"\n",
    "finetuning_dataset_hf = load_dataset(\n",
    "    \"json\",\n",
    "    data_files=path,\n",
    "    split=\"train\"\n",
    ")\n",
    "\n",
    "tokenized_dataset = finetuning_dataset_hf.map(\n",
    "    tokenize_datapoint,\n",
    "    batched=True,\n",
    "    batch_size=1,\n",
    "    drop_last_batch=True\n",
    ")\n",
    "tokenized_dataset = tokenized_dataset.add_column(\"labels\", tokenized_dataset[\"input_ids\"])\n",
    "splitted_dataset = tokenized_dataset.train_test_split(test_size=0.1, shuffle=True, seed=123)\n",
    "\n",
    "# return splitted_dataset[\"train\"], splitted_dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['id', 'input', 'output', 'input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 15\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(splitted_dataset[\"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'create_token'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcreate_token\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'create_token'"
     ]
    }
   ],
   "source": [
    "import create_token\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'data_generate'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mdata_generate\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'data_generate'"
     ]
    }
   ],
   "source": [
    "import data_generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'FT_LLM'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mFT_LLM\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m create_token\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'FT_LLM'"
     ]
    }
   ],
   "source": [
    "from FT_LLM import create_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'data'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfinal\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m finetuning_dataset_hf\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'data'"
     ]
    }
   ],
   "source": [
    "from data.final import finetuning_dataset_hf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'create_token'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcreate_token\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m the_final_dataset\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'create_token'"
     ]
    }
   ],
   "source": [
    "from create_token import the_final_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'create_token'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcreate_token\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'create_token'"
     ]
    }
   ],
   "source": [
    "import create_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/home/jeet/Jeet/FT_LLM/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from create_token import main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<function main at 0x7fc56cf5a0e0>\n"
     ]
    }
   ],
   "source": [
    "print(main)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'create_token' has no attribute 'thisis'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[56], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mcreate_token\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mthisis\u001b[49m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'create_token' has no attribute 'thisis'"
     ]
    }
   ],
   "source": [
    "print(create_token.thisis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[57], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m (\u001b[43mDataset\u001b[49m({\n\u001b[1;32m      2\u001b[0m     features: [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m      3\u001b[0m     num_rows: \u001b[38;5;241m135\u001b[39m\n\u001b[1;32m      4\u001b[0m }), Dataset({\n\u001b[1;32m      5\u001b[0m     features: [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m      6\u001b[0m     num_rows: \u001b[38;5;241m15\u001b[39m\n\u001b[1;32m      7\u001b[0m }))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Dataset' is not defined"
     ]
    }
   ],
   "source": [
    "(Dataset({\n",
    "    features: ['id', 'input', 'output', 'input_ids', 'attention_mask', 'labels'],\n",
    "    num_rows: 135\n",
    "}), Dataset({\n",
    "    features: ['id', 'input', 'output', 'input_ids', 'attention_mask', 'labels'],\n",
    "    num_rows: 15\n",
    "}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
